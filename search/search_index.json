{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>LCLStreamer is application that reads LCLS data in parallel as fast as possible. After some rapid calibration and/or pre-processing, the data is serialized, accumulated and finally passed to one or more handlers that transfer it to external applications.</p> <p>LCLStreamer has several components:</p> <ul> <li>An event source (that generates the data)</li> <li>A processing pipeline (for data reduction, pre-processing, etc.)</li> <li>A serializer (that turns the data into a binary blob of bytes)</li> <li>One or more handlers (that writes the bytes to files, sends them out  through   sockets, etc)</li> </ul> <p>The data flows through all the componets in the following way:</p> <pre><code>stateDiagram-v2\n    direction LR\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    C --&gt; E\n    C --&gt; F\n    C --&gt; G\n    A: Source\n    B: Processing Pipeline\n    C: Serializer\n    D: Handler 1\n    E: Handler 2\n    F: ...\n    G: ...</code></pre> <p>LCLStreamer can be run using different implementations of all its components, depending on the data source, the type of preprocessing that is peformed on the data, and the format and final destination (a file, a network socket) of the processed data.</p> <ul> <li> <p>Please see how to install and run LCLStreamer here:   Installation / Running</p> </li> <li> <p>Please see how to configure LCLStreamer here: Configuration</p> </li> <li> <p>For an in-depth discussion of the data flow through the LCLStreamer application, please   see here: LCLStreamer Data Workflow</p> </li> </ul>"},{"location":"configuration_data_handlers/","title":"Configuration: Data Handlers","text":""},{"location":"configuration_data_handlers/#binaryfilewritingdatahandler","title":"BinaryFileWritingDataHandler","text":"<p>This Data Handler class writes serialized data into a file. The name of the file is generated by joining a string identifying the rank of the process writing the file, with a number that increases progressively with each file written by the same process. A suffix is appended to the resulting string. Optionally, a prefix can be prepended to the file name.</p>"},{"location":"configuration_data_handlers/#configuration-parameters-for-hdf5serializer","title":"Configuration Parameters for Hdf5Serializer","text":"<ul> <li> <p><code>file_prefix</code> (str): This parameter is optional. It defines a prefix that is prepended   to the name of each file written by the Data Handler. The default value is \"\" (an   empty string, no prefix). Example: <code>processed_</code></p> </li> <li> <p><code>file_suffix</code> (str): This parameter is optional.  It defines the suffix (extension)   for the files written by the Data Handler. The default value is \"h5\" (i.e: an \".h5\"   extension is used for the files). Example: <code>.data</code></p> </li> <li> <p><code>write_directory</code> (str): This parameter is optional. It defines the directory where   the Data Handler writes the files. The default value of this parameter is the current   working directory.</p> </li> </ul>"},{"location":"configuration_data_handlers/#binaryfilewritingdatahandler_1","title":"BinaryFileWritingDataHandler","text":"<p>This Data Handler sends data through a socket, to be consumed by an external application. The handler can create sockets using either the <code>NNG</code> or the <code>ZMQ</code> library. Currently, Sockets created by this handler are blocking: if nothing receives the data on the other side of the socket, the data flow through the handler stops.</p>"},{"location":"configuration_data_handlers/#configuration-parameters-for-hdf5serializer_1","title":"Configuration Parameters for Hdf5Serializer","text":"<ul> <li> <p><code>urls</code> (list of str): This parameter defines the list of URIS where the Data Handler   opens its sockets. Each URI must be in a format compatible with the <code>ZMQ</code> and <code>NNG</code>   libraries, depending on which one is used by the handler. Additionally, this   parameter must always have the form of a list, even if just one URI is provided</p> </li> <li> <p><code>role</code> (str): This parameter specifies if the Data Handler should create a socket or   connect to an already existing one. The parameter can only have two values: <code>server</code>,   (to have the handler create the socket) or <code>client</code> (to connect to an existing   socket. The <code>server</code> role corresponds to sockets performing a <code>bind</code> operation with   the <code>ZMQ</code> library, or a <code>listen</code> operation with the <code>NNG</code> library. The <code>client</code> role   corresponds instead to socket performing a <code>connect</code> operation with <code>ZMQ</code> or a <code>dial</code>   operation with <code>NNG</code>.</p> </li> <li> <p><code>library</code> (str): This parameter dictates which library the handler uses to manage the   socket. The parameter can only have the values <code>zmq</code> or <code>nng</code> to use the <code>ZMQ</code> or <code>NNG</code>   libraries respectively</p> </li> <li> <p><code>socket_type</code> (str): This parameter determines the type of socket that the handler   creates. Currently only the <code>PUSH</code> socket type is supported and this parameter can   only take the value <code>push</code></p> </li> </ul>"},{"location":"configuration_data_serializers/","title":"Configuration: Data Serializers","text":""},{"location":"configuration_data_serializers/#hdf5dataserializer","title":"Hdf5DataSerializer","text":"<p>This Data Serializer class turns the Data accumulated by LCLStreamer into a binary blob with the internal structure of an HDF5 file.</p>"},{"location":"configuration_data_serializers/#configuration-parameters-for-hdf5serializer","title":"Configuration Parameters for Hdf5Serializer","text":"<ul> <li> <p><code>compression</code> (str): This parameter is optional. If present, the HDF5 Data Serializer   compresses the data with the specified algorithm during serialization. Possible values   are: <code>gzip</code>, <code>gzip_with_shuffle</code> (byteshuffle is performed before gzip compression is   applied) <code>bitshuffle_with_lz4</code>, <code>bitshuffle_with_zstd</code> (bitshuffle is peformed before   LZ4 or Zstd comnpressionis applied), and <code>zfp</code>. If this parameter is not specified,   or is set to <code>null</code>, no compression is applied during serialization. The default value   of this parameter is <code>null</code>. Example: <code>gzip</code></p> </li> <li> <p><code>compression_level</code> (int): This parameter is optional. If the compression algorithm   specified by the <code>compression</code> configuration parameter supports compression levels,   this parameter specfies the compression level applied during serialization. The   default value of this parameter is 3. Example: <code>5</code></p> </li> <li> <p><code>fields</code> (dict of str): this entry is a dictionary that specified where each data   source will be stored in the internal structure of the HDF5 file. Each key in the dictionary is the   name of a data source, and each value is the internal path where the data source is   stored. If a data source is not present in the dictionary, it is excluded from the   serialization process and does not appear in the binary blob containing the serialized   data. Example:</p> </li> </ul> <pre><code>fields:\n  timestamp: /data/timestamp\n  detector_data: /data/data\n</code></pre>"},{"location":"configuration_data_sources/","title":"Configuration: Data Sources","text":""},{"location":"configuration_data_sources/#psana1areadetector","title":"Psana1AreaDetector","text":"<p>This Data Source class retrieves a detector data frame from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The data frame is returned as a 2d numpy array of type <code>int</code> (when the data is not   calibrated) or <code>float</code> (when the data is calibrated)</p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1areadetector","title":"Configuration parameters for Psana1AreaDetector","text":"<ul> <li> <p><code>psana_name</code> (str): The name (or alias) of the area detector in the psana1 framework</p> </li> <li> <p><code>calibration</code> (bool): When the value of this parameter is <code>True</code>, the Data Source   class retrieves calibrated data frames. When the value is instead <code>False</code>, the   data is retrieved in raw form.</p> </li> </ul>"},{"location":"configuration_data_sources/#psana1assmebledareadetector","title":"Psana1AssmebledAreaDetector","text":"<p>This Data Source class retrieves a detector data frame from a psana1 data event, in calibrated form, and with geometry information already applied to it. This is the closest approximation to the physical appearance of the area detector.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The data frame is returned as a 2d numpy array of type <code>float</code></p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1assmebledareadetector","title":"Configuration parameters for Psana1AssmebledAreaDetector","text":"<ul> <li><code>psana_name</code> (str): The name (or alias) of the area detector in the psana1 framework</li> </ul>"},{"location":"configuration_data_sources/#psana1pv","title":"Psana1PV","text":"<p>This Data Source class retrieves the value of an Epics variable from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The variable value is returned in the form of an numpy array of type <code>int</code>, <code>float</code> or   <code>str</code> type, depending on the value of the Epics variable</p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1pv","title":"Configuration parameters for Psana1PV","text":"<ul> <li><code>psana_name</code> (str): The name (or alias) of the Epics variable in the psana1   framework</li> </ul>"},{"location":"configuration_data_sources/#psana1bbmondetectortotalintensity","title":"Psana1BbmonDetectorTotalIntensity","text":"<p>This Data Source class retrieves the total intensity recorded by a BBmon detector from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The variable value is returned in the form of an numpy array of type <code>float</code></p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1bbmondetectortotalintensity","title":"Configuration parameters for Psana1BbmonDetectorTotalIntensity","text":"<ul> <li><code>psana_name</code> (str): The name (or alias) of the BBmon detector in the psana1 framework</li> </ul>"},{"location":"configuration_data_sources/#psana1ipmdetector","title":"Psana1IpmDetector","text":"<p>This Data Source class retrieves IPM detector data from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The data frame is returned as an array of type <code>float</code></p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1ipmdetecto","title":"Configuration parameters for Psana1IpmDetecto","text":"<ul> <li> <p><code>psana_name</code> (str): The name (or alias) of the IPM detector in the psana1 framework</p> </li> <li> <p><code>psana_function</code> (str): The name of the psana1 function used to recover data from   the IPM detector. Currently supported values are:</p> <ul> <li><code>channel</code>: this function returns channel information from the detector in a 2d   array. The first axis of the array represents the channel number, while the   data points from each channel are encoded along the second axis</li> </ul> </li> </ul>"},{"location":"configuration_data_sources/#psana1usdusbdetector","title":"Psana1UsdUsbDetector","text":"<p>This Data Source class retrieves UsdUsb detector data from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The data frame is returned as an array of type <code>float</code></p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1usdusbdetector","title":"Configuration parameters for Psana1UsdUsbDetector","text":"<ul> <li> <p><code>psana_name</code> (str): The name (or alias) of the UsbUsd detector in the psana1 framework</p> </li> <li> <p><code>psana_function</code> (str): The name of the psana1 function used to recover data from   the UsbUsd detector. Currently supported values are:</p> <ul> <li><code>values</code>: this function returns the values read by the detector as an array of   type <code>float</code> </li> </ul> </li> </ul>"},{"location":"configuration_data_sources/#psana1evrcodes","title":"Psana1EvrCodes","text":"<p>This Data Source class retrieves EVR event code data from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The data frame is returned as an array of type <code>int</code>. The 1d array has always size    256, since that's the maximum number of EVR codes that can be theoretically   associated with an event. The EVR codes associated with the psana1 event are stored   at the beginning of the array. Any remaining space is filled with the integer value   0 </p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1evrcodes","title":"Configuration parameters for Psana1EvrCodes","text":"<ul> <li><code>psana_name</code> (str): The name (or alias) of the EVR data source in the psana1   framework</li> </ul>"},{"location":"configuration_data_sources/#psana1timestamp","title":"Psana1Timestamp","text":"<p>This Data Source class retrieves timestamp information from a psana1 data event.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The timestamp information is returned as a numpy array of type <code>float64</code></p> </li> </ul>"},{"location":"configuration_data_sources/#genericrandomnumpyarray","title":"GenericRandomNumpyArray","text":"<p>This Data Source class generates a random numerical array of size and type chosen by the user.</p> <ul> <li> <p>This Data Source is compatible with the following Event Source classes:   <code>Psana1EventSource</code></p> </li> <li> <p>The random array has size and type chosen by the user</p> </li> </ul>"},{"location":"configuration_data_sources/#configuration-parameters-for-psana1evrcodes_1","title":"Configuration parameters for Psana1EvrCodes","text":"<ul> <li> <p><code>array_shape</code> (list of int): the shape of the generated array, with components that   follow the same format as numpy's <code>shape</code> obejcts</p> </li> <li> <p><code>array_dtype</code> (str): the numerical type of the generated array, in the same format   as numpy's numerical <code>dtypes</code></p> </li> </ul>"},{"location":"configuration_event_sources/","title":"Configuration: Event Sources","text":""},{"location":"configuration_event_sources/#psana1eventsource","title":"Psana1EventSource","text":"<p>This class retrieves data events from the psana1 software framework in use at the LCLS facility.</p> <ul> <li>The <code>source_identifier</code> for the this event source is a text string that identifies   a run within a specific LCLS experiment in the psana1 framework (e.g.:   exp=xpptut15:run=430 )</li> </ul>"},{"location":"configuration_lclstreamer/","title":"Configuration: LCLStreamer","text":""},{"location":"configuration_lclstreamer/#configuring-the-lclstreamer-application","title":"Configuring the LCLStreamer Application","text":"<p>The behavior of the LCLStreamer application is fully determined by the content of its configuration file. This file determines which implementation of the different workflow components should be used, how each component is configured, and which data elements should be retrieved and processed by the application. Additonally, it specifies where the data should come from.</p> <p>LCLStreamer reads the <code>lclstreamer</code> section of the configuration files to determine which implementation of each component it should use. Each component entry in this section identifies a python class that implements the operations required by the component. For example:</p> <pre><code>lclstreamer:\n  [...]\n  event_source: Psana1EventSource\n  processing_pipeline: NoOpProcessingPipeline\n  data_serializer: Hdf5Serializer\n  data_handlers:\n    - BinaryFileWritingDataHandler\n    - BinaryDataStreamingDataHandler\n</code></pre> <p>With this configuration options, LCLStreamer reads psana1 data (<code>Psana1EventSource</code>), does not perform any processing of the data (<code>NoOpProcessingPipeline</code>), serializes the data in a binary blob with the internal structure of an HDF5 file (<code>Hdf5Serializer</code>) and finally hands the binary blob to two data handlers: one that saves it as a file (<code>BinaryFileWritingDataHandler</code>) and one that streams it through a network socket (<code>BinaryDataStreamingDataHandler</code>)</p>"},{"location":"configuration_lclstreamer/#configuring-lclstreamers-components","title":"Configuring LCLStreamer's components","text":"<p>Configuration options can be provided for each of the Python classes that implement the LCLStreamer components. The configuration options for a specific class are defined in an entry with the same name as the class located in a specific section of the configuration file (<code>event source</code>, <code>processing_pipeline</code>, <code>data_serializer</code>, or <code>data_handlers</code> depending on the component that the class implements)</p> <p>For example, the configuration parameters for the <code>Hdf5Serializer</code> class, which implements the <code>data serializer</code> component, are defined by the <code>Hdf5Serializer</code> entry in the <code>data_serializer</code> section of the configuration file:</p> <pre><code>data_serializer:\n    Hdf5Serializer:\n        compression_level: 3\n        compression: zfp\n        fields:\n            timestamp: /data/timestamp\n            detector_data: /data/data\n</code></pre> <p>A configuration entry for each of the classes mentioned in the <code>lclstreamer</code> section must be present somewhere in the configuration file (in the relevant section). The entry must be present even if the class takes no configuration options at all. In that case, the following syntax must be used:</p> <pre><code>processing_pipeline:\n    NoOpProcessingPipeline: {}\n</code></pre>"},{"location":"configuration_lclstreamer/#configuring-the-data-sources","title":"Configuring the data sources","text":"<p>The <code>data_sources</code> section of the configuration file defines the data that LCLStreamer extracts from every data event it processes. If a piece of information is part of the data event, but not included in the <code>data_sources</code> section, LCLStreamer justs ignores it.</p> <p>The <code>data sources</code> section of the configuration file consists of a dictionary of data sources. Each entry has a key, which acts as a name that identifies the extracted data throughout the whole LCLStreamer data workflow, and a value, which is itelf a dictionary. This inner dictionary defines the nature of the data source (via the mandatory <code>type</code> entry) and any other parameters needed to configure it. The <code>type</code> of a data source is the name of the Python class that implements it. For example:</p> <pre><code>data_sources:\n    timestamp:\n        type: Psana1Timestamp\n\n    detector_data:\n        type: Psana1AreaDetector\n        psana_name: Jungfrau1M\n        calibration: true\n</code></pre> <p>This snippet of the configuration files defines two data sources, one called <code>timestamp</code> and one called <code>detector_data</code>.  The <code>timestamp</code> data class is of type <code>Psana1Timestamp</code>. This means that a Python class of the same name determines how this type of data is extracted from a data event. The <code>detector_data</code> class is instead of type <code>Psana1AreaDetector</code>. The two configuration parameters <code>psana_name</code> and <code>calibration</code> are passed to the Python class <code>Psana1AreaDetector</code> that defines how this type of data is retrieved.</p>"},{"location":"configuration_lclstreamer/#available-event-sources-processing-pipelines-data-handlers-and-data-sources","title":"Available Event Sources, Processing Pipelines, Data Handlers and Data Sources","text":"<ul> <li> <p>For a list of all available Event Source python classes, and their configuration   parameters, see here: Configuration: Event Sources</p> </li> <li> <p>For a list of all available Processing Pipeline classes, and their configuration   parameters, see here:   Configuration: Processing Pipelines</p> </li> <li> <p>For a list of all available Serializer classes, and their configuration   parameters, see here:   Configuration: Data Serializers</p> </li> <li> <p>For a list of all available Data Handler classes, and their configuration   parameters, see here: Configuration: Data Handlers</p> </li> <li> <p>For a list of all available Data Source classes, and their configuration parameters,   see here: Configuration: Data Sources</p> </li> </ul>"},{"location":"configuration_processing_pipelines/","title":"Configuration: Event Sources","text":""},{"location":"configuration_processing_pipelines/#noopprocessing-pipeline","title":"NoOpProcessing Pipeline","text":"<p>This is a NOOP processing pipeline. This pipeline merely accumulates the data without modifying it.</p> <ul> <li>The NOOP processing pipeline does not require any configuration parameters.</li> </ul>"},{"location":"installation_running/","title":"Installation / Running","text":""},{"location":"installation_running/#installing-lclstreamer","title":"Installing LCLStreamer","text":"<p>LCLStreamer uses pixi for deployment, due to the mixture of conda and PyPI packages needed by the application.</p> <p>In order to install LCLStreamer, first install pixi, then run the following command from the top level folder of the GitHub repository:</p> <pre><code>pixi install\n</code></pre> <p>This will create a virtual environment, located in the <code>.pixi</code> subdirectory, that contains all the python packages required by LCLStreamer to run, each at a version that is compatible withthe LCLStreamer application.</p>"},{"location":"installation_running/#running-lclstreamer","title":"Running LCLStreamer","text":"<p>LCLStreamer is currently designed to run exclusively using the MPI protocol. After installing LCLStreamer, the appliaction can be launched using the <code>pixi run</code> command. For example:</p> <pre><code>pixi run mpirun -n 8 lclstreamer\n</code></pre> <p>LCLStreamer looks for a configuration file named <code>lclstreamer.yaml</code> in the current working directory. Alternatively, the path to the configuration file can be passed to the lclstreamer executable using the --config option:</p> <pre><code>pixi run mpirun -n 8 lclstreamer --config examples/lclstreamer.yaml\n</code></pre>"},{"location":"lclstreamer_data_workflow/","title":"LCLStreamer Data Workflow","text":"<p>The LCLStreamer application extracts data from events retrieved from an Event Source.</p> <p>The application retrieves a single event from the Event Source, and extracts all the required data from the event (Call to the <code>get_events</code> method of a <code>DataSource</code> class.) Only data entries listed in the <code>data_sources</code> section of the configuration file are retrieved from each event. Any other data is simply discared. The data retrieved for each event has the format of a Python dictionary. Each key in the dictionary corresponds to a data source. The value associated with the key is instead the information retrieved from the data source for the event being processed.</p> <p>The operations of a Processing Pipeline are then applied to the data retrieved from each event (Call to the <code>process_data</code> method of a <code>ProcessingPipeline</code> class). The results of processing several consecutive events are accumulated internally, until a number of events matching the batch size parameter is reached. At that point, the accumulated data is returned in bulk (Call to the <code>collect_data</code> method of a <code>ProcessingPipeline</code> class). The data still has the format of a python dictionary, which each key representing a data entry, and the corresponding value storing the accumulated data.</p> <p>The data is then serialized into a binary form (Call to the <code>serialize_data</code> function of a <code>DataSerializer</code> class. After being serialized, the data has the format of a binary blob.</p> <p>Finally, the data is passed to one or more Data Handlers, that can foward the data to the filesystem or other external applications. If multiple Data Handlers are present, they handle the same binary blob in sequence (Call to the <code>handle_data</code> function of a <code>DataHandler</code> class): the binary data is not modified at all as it flows through the Data Handlers.</p>"}]}